{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Identifying Individuals Based Off Of Their Accelerometer Data Using The WISDM Data Set</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "columns = ['user','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "\n",
    "# save this for making a new data set if need be\n",
    "# load in data set\n",
    "df = pd.read_csv('WISDM_at_v2.0_raw.txt', header = None, names = columns)\n",
    "\n",
    "# get rid of unnecessary characters\n",
    "df['z-axis'] = df['z-axis'].str.rstrip(';')\n",
    "\n",
    "# obtain only walking data since this has the most of any activity\n",
    "walking = df.loc[df.activity == 'Walking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1679,  599,  685,  669, 1277,  674,  594,  678,  648,  584,  582,\n",
       "        636, 1758,  708,  711,  687,  563,  621,  623,  720, 1793,  568,\n",
       "        640,  671,  694,  664,  585,  684, 1480,  655, 1603,  651,  579,\n",
       "        613,  590,  639,  587,  719,  635, 1742,  624,  710,  676,  693,\n",
       "        646,  713,  610,  615,  705,  653,  702,  604,  618,  654,  656,\n",
       "        606,  998,  586, 1319,  668, 1768, 1100,  573, 1491,  712, 1518,\n",
       "        730,  622,  647, 1727, 1477,  588,  634,  661,  686,  690,  709,\n",
       "        663,  597, 1656,  630,  616,  691,  625,  612,  650,  658, 1750,\n",
       "        598,  729,  714,  607,  628,  589,  728,  925,  593,  600,  637,\n",
       "        641,  609,  633, 1783,  688,  605, 1247,  695,  595,  602,  194,\n",
       "        583,  716,  727, 1797, 1320, 1676,  673, 1802,  611, 1117,  689,\n",
       "       1774,  666,  617,  706,  627,  675, 1799, 1703,  670, 1759, 1554,\n",
       "       1778, 1064, 1238,  726,  632,  608,  725,  614,  697, 1775,  592,\n",
       "        723,  591, 1512, 1253,  580,  703,  722,  626,  561,  619,  652,\n",
       "        683,  717,  567,  601,  642,  577, 1736,  724,  715,  696,  665,\n",
       "        638,  581,  682,  660, 1104,  718,  704,  672,  721,  645,  603,\n",
       "        596, 1767, 1559,  631,  578,  659, 1274,  667,  681,  649,  565,\n",
       "        679,  562,  692,  707,  657])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique id's before getting rid of those without a sufficient amount of data\n",
    "walking.user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain only users who have over x minutes of data\n",
    "minDataPoints = 16000\n",
    "\n",
    "for i in walking.user.unique():    \n",
    "    if len(walking[walking.user == i]) < minDataPoints: \n",
    "        walking = walking[walking.user != i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 669  648  585 1750  641  688  602  675 1238  608  603  679]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Unique users after stripping those who don't have enough data points\n",
    "print(walking.user.unique())\n",
    "print(len(walking.user.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of unnecessary characters and rows with empty entries\n",
    "walking = walking[walking.columns.unique()].replace(';', '')\n",
    "walking = walking.dropna()\n",
    "\n",
    "# update columns to be usable types\n",
    "walking.timestamp = walking.timestamp.astype(int)\n",
    "walking['x-axis'] = walking['x-axis'].astype(float)\n",
    "walking['y-axis'] = walking['y-axis'].astype(float)\n",
    "walking['z-axis'] = walking['z-axis'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize time stamps starting at 0 milliseconds\n",
    "def normalize_time_stamps(df):\n",
    "\n",
    "    # iterate through all users\n",
    "    for user in df.user.unique():\n",
    "\n",
    "        # iterate through all activities\n",
    "        for activity in df.activity.unique():\n",
    "\n",
    "            key = (df.user == user) & (df.activity == activity)\n",
    "            # obtain all rows for a particular user and an activity\n",
    "            user_activity_to_process = df[key]\n",
    "            \n",
    "            # if there are no rows for this activity, drop the activity from that user\n",
    "            if len(user_activity_to_process.timestamp) == 0:\n",
    "                df[(df.user == user)] = df[(df.user == user) & (df.activity != activity)]\n",
    "                break\n",
    "\n",
    "            # otherwise normalize the time stamps to start at 0, and be in milliseconds?\n",
    "            else:\n",
    "                df.loc[key, 'timestamp'] -= user_activity_to_process.timestamp.iloc[0]\n",
    "                df.loc[key, 'timestamp'] /= 1000\n",
    "                print(df[key])\n",
    "                            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user activity  timestamp  x-axis  y-axis  z-axis\n",
      "39169   669  Walking       0.00    6.09    3.99   -0.04\n",
      "39170   669  Walking       0.05    6.36    3.68   -0.27\n",
      "39171   669  Walking       0.10    7.63    4.63   -0.42\n",
      "39172   669  Walking       0.15    7.63    5.71   -0.53\n",
      "39173   669  Walking       0.20    9.04    6.47    0.08\n",
      "...     ...      ...        ...     ...     ...     ...\n",
      "58557   669  Walking     716.60    4.14   11.03    1.12\n",
      "58558   669  Walking     716.60    4.14   11.03    1.12\n",
      "58559   669  Walking     716.60    4.14   11.03    1.12\n",
      "58560   669  Walking     716.60    4.14   11.03    1.12\n",
      "58561   669  Walking     716.60    4.14   11.03    1.12\n",
      "\n",
      "[16532 rows x 6 columns]\n",
      "        user activity  timestamp  x-axis  y-axis     z-axis\n",
      "112345   648  Walking       0.00   -2.11    6.47   5.284695\n",
      "112346   648  Walking       0.05   -4.25    8.12  10.569390\n",
      "112347   648  Walking       0.10    3.15    2.07   5.434519\n",
      "112348   648  Walking       0.15   -0.61    4.60   5.325556\n",
      "112349   648  Walking       0.20    3.06    2.98   7.777218\n",
      "...      ...      ...        ...     ...     ...        ...\n",
      "133904   648  Walking     954.55  -10.65    9.81 -11.223166\n",
      "133905   648  Walking     954.55  -10.65    9.81 -11.223166\n",
      "133906   648  Walking     954.55  -10.65    9.81 -11.223166\n",
      "133907   648  Walking     954.55  -10.65    9.81 -11.223166\n",
      "133908   648  Walking     954.55  -10.65    9.81 -11.223166\n",
      "\n",
      "[17000 rows x 6 columns]\n",
      "        user activity  timestamp    x-axis    y-axis     z-axis\n",
      "316949   585  Walking       0.00  3.173541 -0.694638   9.152874\n",
      "316950   585  Walking       0.05  3.146300 -0.340509   9.193735\n",
      "316951   585  Walking       0.10  1.184970 -0.040861  10.841797\n",
      "316952   585  Walking       0.15  2.792171  0.653777  10.310603\n",
      "316953   585  Walking       0.20  2.220117  0.503953  10.106298\n",
      "...      ...      ...        ...       ...       ...        ...\n",
      "342233   585  Walking    1333.50 -0.463092  5.706926   8.349273\n",
      "342234   585  Walking    1333.55 -1.607201  5.516241   8.621680\n",
      "342235   585  Walking    1333.60 -1.307553  5.625204   8.539958\n",
      "342236   585  Walking    1333.65 -1.334794  5.788648   9.003050\n",
      "342237   585  Walking    1333.70 -1.225831  6.020194   8.471856\n",
      "\n",
      "[21886 rows x 6 columns]\n",
      "         user activity  timestamp    x-axis     y-axis     z-axis\n",
      "1492383  1750  Walking      0.000 -3.823853  12.672226   0.331696\n",
      "1492384  1750  Walking      0.050  4.261078   4.433762  10.710114\n",
      "1492385  1750  Walking      0.100  0.363220   1.526138   1.949158\n",
      "1492386  1750  Walking      0.150 -1.498230   1.722519  -0.463348\n",
      "1492387  1750  Walking      0.200 -2.353973   2.699661  -4.098175\n",
      "...       ...      ...        ...       ...        ...        ...\n",
      "1534378  1750  Walking  48676.379  0.068054  -9.619949   1.486176\n",
      "1534379  1750  Walking  48676.429  0.004974  -9.487839   1.720642\n",
      "1534380  1750  Walking  48676.479  0.020447  -9.542587   1.484985\n",
      "1534381  1750  Walking  48676.529  0.034729  -9.554489   1.561157\n",
      "1534382  1750  Walking  48676.577  0.039490  -9.483078   1.382629\n",
      "\n",
      "[42000 rows x 6 columns]\n",
      "         user activity  timestamp    x-axis    y-axis    z-axis\n",
      "1655816   641  Walking       0.00  0.653777  1.648062  9.575105\n",
      "1655817   641  Walking       0.05  0.572055  1.648062  9.425281\n",
      "1655818   641  Walking       0.10  0.612916  1.838747  9.466142\n",
      "1655819   641  Walking       0.15  0.885323  1.648062  9.656827\n",
      "1655820   641  Walking       0.20  2.533385  2.070293  8.853226\n",
      "...       ...      ...        ...       ...       ...       ...\n",
      "1674263   641  Walking    1203.50 -0.694638  3.595772  8.771504\n",
      "1674264   641  Walking    1203.55 -0.885323  3.568531  8.390134\n",
      "1674265   641  Walking    1203.65 -0.953424  3.568531  9.112013\n",
      "1674266   641  Walking    1203.70 -0.926184  3.677494  9.847511\n",
      "1674267   641  Walking    1203.75 -0.422231  3.949901  9.043911\n",
      "\n",
      "[18452 rows x 6 columns]\n",
      "         user activity  timestamp  x-axis  y-axis    z-axis\n",
      "1711818   688  Walking       0.00   -0.04    8.62  2.873893\n",
      "1711819   688  Walking       0.05    0.69    9.53  4.181447\n",
      "1711820   688  Walking       0.10   -0.76    8.92  4.140586\n",
      "1711821   688  Walking       0.15   -1.57    8.58  3.636633\n",
      "1711822   688  Walking       0.20   -1.08    8.35  3.786457\n",
      "...       ...      ...        ...     ...     ...       ...\n",
      "1731683   688  Walking     708.35    2.45    9.30  0.762740\n",
      "1731684   688  Walking     708.35    2.45    9.30  0.762740\n",
      "1731685   688  Walking     708.35    2.45    9.30  0.762740\n",
      "1731686   688  Walking     708.35    2.45    9.30  0.762740\n",
      "1731687   688  Walking     708.35    2.45    9.30  0.762740\n",
      "\n",
      "[19870 rows x 6 columns]\n",
      "         user activity  timestamp    x-axis     y-axis    z-axis\n",
      "1782412   602  Walking       0.00  0.149824   5.829509 -0.885323\n",
      "1782413   602  Walking       0.05  1.225831   6.633110 -0.081722\n",
      "1782414   602  Walking       0.10  6.170018  11.345750  0.149824\n",
      "1782415   602  Walking       0.15  6.401564  10.337844 -0.994285\n",
      "1782416   602  Walking       0.20  3.949901  11.727119 -1.334794\n",
      "...       ...      ...        ...       ...        ...       ...\n",
      "1804056   602  Walking    1106.80  1.035146   5.979332  7.858941\n",
      "1804057   602  Walking    1106.85  2.111154   8.580819  6.047434\n",
      "1804058   602  Walking    1106.90  1.838747   6.701211  8.853226\n",
      "1804059   602  Walking    1106.95  1.757025   7.927043  7.777218\n",
      "1804060   602  Walking    1107.00  1.607201   7.395849  7.818079\n",
      "\n",
      "[16008 rows x 6 columns]\n",
      "         user activity  timestamp  x-axis  y-axis  z-axis\n",
      "2044043   675  Walking       0.00    5.67    6.74    1.65\n",
      "2044044   675  Walking       0.05    9.85   13.67   -7.25\n",
      "2044045   675  Walking       0.10   -1.12   11.30  -10.50\n",
      "2044046   675  Walking       0.15   -0.38    0.04   -0.57\n",
      "2044047   675  Walking       0.20    1.92    4.37   -6.13\n",
      "...       ...      ...        ...     ...     ...     ...\n",
      "2063190   675  Walking     628.85    6.32   -0.89   -9.30\n",
      "2063191   675  Walking     628.85    6.32   -0.89   -9.30\n",
      "2063192   675  Walking     628.85    6.32   -0.89   -9.30\n",
      "2063193   675  Walking     628.85    6.32   -0.89   -9.30\n",
      "2063194   675  Walking     628.85    6.32   -0.89   -9.30\n",
      "\n",
      "[19152 rows x 6 columns]\n",
      "         user activity    timestamp    x-axis     y-axis    z-axis\n",
      "2189151  1238  Walking        0.000  0.481763  -0.960870 -0.370048\n",
      "2189152  1238  Walking        0.050 -3.960699   0.604336  0.816441\n",
      "2189153  1238  Walking        0.100 -2.920429   1.438897  1.006162\n",
      "2189154  1238  Walking        0.150 -2.015523  -1.302469  0.530793\n",
      "2189155  1238  Walking        0.200 -2.504748   0.271792  1.220398\n",
      "...       ...      ...          ...       ...        ...       ...\n",
      "2238595  1238  Walking  5785081.598  2.413355 -16.203957 -0.076614\n",
      "2238596  1238  Walking  5785081.648  4.865018  -8.963891 -3.600879\n",
      "2238597  1238  Walking  5785081.698  2.758120  -4.137180 -0.344765\n",
      "2238598  1238  Walking  5785081.748 -2.643199 -10.228029  4.750096\n",
      "2238599  1238  Walking  5785081.797 -4.788404  -9.615114 -0.497994\n",
      "\n",
      "[34121 rows x 6 columns]\n",
      "         user activity  timestamp  x-axis  y-axis  z-axis\n",
      "2306157   608  Walking       0.00   -2.11    8.96    0.08\n",
      "2306158   608  Walking       0.05   -0.57    8.28   -0.04\n",
      "2306159   608  Walking       0.10   -0.46    7.86   -0.11\n",
      "2306160   608  Walking       0.15    0.00    6.02    0.27\n",
      "2306161   608  Walking       0.20   -1.57    6.09    1.50\n",
      "...       ...      ...        ...     ...     ...     ...\n",
      "2329612   608  Walking    1206.55   -0.99    4.25   -1.65\n",
      "2329613   608  Walking    1206.60   -1.80    4.82   -0.46\n",
      "2329614   608  Walking    1206.65   -3.38    4.63   -0.76\n",
      "2329615   608  Walking    1206.70    2.22   17.73   16.51\n",
      "2329616   608  Walking    1206.75   -5.09   15.13    3.17\n",
      "\n",
      "[16672 rows x 6 columns]\n",
      "         user activity    timestamp    x-axis     y-axis    z-axis\n",
      "2758469   603  Walking        0.000  0.844462   8.008764  2.792171\n",
      "2758470   603  Walking        0.050  1.116869   8.621680  3.786457\n",
      "2758471   603  Walking        0.100 -0.503953  16.657684  1.307553\n",
      "2758472   603  Walking        0.150  4.794363  10.760075 -1.184970\n",
      "2758473   603  Walking        0.200 -0.040861   9.234595 -0.694638\n",
      "...       ...      ...          ...       ...        ...       ...\n",
      "2783058   603  Walking  5165610.220  0.885323  12.598822  0.149824\n",
      "2783059   603  Walking  5165610.221  0.081722   9.806650  1.757025\n",
      "2783060   603  Walking  5165610.222 -4.630918   6.851035  4.821603\n",
      "2783061   603  Walking  5165610.223  3.718355  -3.636633  8.689782\n",
      "2783062   603  Walking  5165610.224  5.284695  -0.190685  8.267551\n",
      "\n",
      "[19773 rows x 6 columns]\n",
      "         user activity  timestamp  x-axis  y-axis  z-axis\n",
      "2917653   679  Walking       0.00   -2.76    5.05   -0.53\n",
      "2917654   679  Walking       0.05   -0.93    4.90    0.57\n",
      "2917655   679  Walking       0.10    5.79   11.03    0.23\n",
      "2917656   679  Walking       0.15    7.40   11.26    0.69\n",
      "2917657   679  Walking       0.20    6.85   11.65    1.92\n",
      "...       ...      ...        ...     ...     ...     ...\n",
      "2954527   679  Walking    1563.75    1.80    5.52    4.75\n",
      "2954528   679  Walking    1563.75    1.80    5.52    4.75\n",
      "2954529   679  Walking    1563.75    1.80    5.52    4.75\n",
      "2954530   679  Walking    1563.75    1.80    5.52    4.75\n",
      "2954531   679  Walking    1563.75    1.80    5.52    4.75\n",
      "\n",
      "[24193 rows x 6 columns]\n",
      "39169      0.00\n",
      "39170      0.05\n",
      "39171      0.10\n",
      "39172      0.15\n",
      "39173      0.20\n",
      "          ...  \n",
      "58557    716.60\n",
      "58558    716.60\n",
      "58559    716.60\n",
      "58560    716.60\n",
      "58561    716.60\n",
      "Name: timestamp, Length: 16532, dtype: float64\n",
      "112345      0.00\n",
      "112346      0.05\n",
      "112347      0.10\n",
      "112348      0.15\n",
      "112349      0.20\n",
      "           ...  \n",
      "133904    954.55\n",
      "133905    954.55\n",
      "133906    954.55\n",
      "133907    954.55\n",
      "133908    954.55\n",
      "Name: timestamp, Length: 17000, dtype: float64\n",
      "316949       0.00\n",
      "316950       0.05\n",
      "316951       0.10\n",
      "316952       0.15\n",
      "316953       0.20\n",
      "           ...   \n",
      "342233    1333.50\n",
      "342234    1333.55\n",
      "342235    1333.60\n",
      "342236    1333.65\n",
      "342237    1333.70\n",
      "Name: timestamp, Length: 21886, dtype: float64\n",
      "1492383        0.000\n",
      "1492384        0.050\n",
      "1492385        0.100\n",
      "1492386        0.150\n",
      "1492387        0.200\n",
      "             ...    \n",
      "1534378    48676.379\n",
      "1534379    48676.429\n",
      "1534380    48676.479\n",
      "1534381    48676.529\n",
      "1534382    48676.577\n",
      "Name: timestamp, Length: 42000, dtype: float64\n",
      "1655816       0.00\n",
      "1655817       0.05\n",
      "1655818       0.10\n",
      "1655819       0.15\n",
      "1655820       0.20\n",
      "            ...   \n",
      "1674263    1203.50\n",
      "1674264    1203.55\n",
      "1674265    1203.65\n",
      "1674266    1203.70\n",
      "1674267    1203.75\n",
      "Name: timestamp, Length: 18452, dtype: float64\n",
      "1711818      0.00\n",
      "1711819      0.05\n",
      "1711820      0.10\n",
      "1711821      0.15\n",
      "1711822      0.20\n",
      "            ...  \n",
      "1731683    708.35\n",
      "1731684    708.35\n",
      "1731685    708.35\n",
      "1731686    708.35\n",
      "1731687    708.35\n",
      "Name: timestamp, Length: 19870, dtype: float64\n",
      "1782412       0.00\n",
      "1782413       0.05\n",
      "1782414       0.10\n",
      "1782415       0.15\n",
      "1782416       0.20\n",
      "            ...   \n",
      "1804056    1106.80\n",
      "1804057    1106.85\n",
      "1804058    1106.90\n",
      "1804059    1106.95\n",
      "1804060    1107.00\n",
      "Name: timestamp, Length: 16008, dtype: float64\n",
      "2044043      0.00\n",
      "2044044      0.05\n",
      "2044045      0.10\n",
      "2044046      0.15\n",
      "2044047      0.20\n",
      "            ...  \n",
      "2063190    628.85\n",
      "2063191    628.85\n",
      "2063192    628.85\n",
      "2063193    628.85\n",
      "2063194    628.85\n",
      "Name: timestamp, Length: 19152, dtype: float64\n",
      "2189151          0.000\n",
      "2189152          0.050\n",
      "2189153          0.100\n",
      "2189154          0.150\n",
      "2189155          0.200\n",
      "              ...     \n",
      "2238595    5785081.598\n",
      "2238596    5785081.648\n",
      "2238597    5785081.698\n",
      "2238598    5785081.748\n",
      "2238599    5785081.797\n",
      "Name: timestamp, Length: 34121, dtype: float64\n",
      "2306157       0.00\n",
      "2306158       0.05\n",
      "2306159       0.10\n",
      "2306160       0.15\n",
      "2306161       0.20\n",
      "            ...   \n",
      "2329612    1206.55\n",
      "2329613    1206.60\n",
      "2329614    1206.65\n",
      "2329615    1206.70\n",
      "2329616    1206.75\n",
      "Name: timestamp, Length: 16672, dtype: float64\n",
      "2758469          0.000\n",
      "2758470          0.050\n",
      "2758471          0.100\n",
      "2758472          0.150\n",
      "2758473          0.200\n",
      "              ...     \n",
      "2783058    5165610.220\n",
      "2783059    5165610.221\n",
      "2783060    5165610.222\n",
      "2783061    5165610.223\n",
      "2783062    5165610.224\n",
      "Name: timestamp, Length: 19773, dtype: float64\n",
      "2917653       0.00\n",
      "2917654       0.05\n",
      "2917655       0.10\n",
      "2917656       0.15\n",
      "2917657       0.20\n",
      "            ...   \n",
      "2954527    1563.75\n",
      "2954528    1563.75\n",
      "2954529    1563.75\n",
      "2954530    1563.75\n",
      "2954531    1563.75\n",
      "Name: timestamp, Length: 24193, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# obtain a data set with normalized time stamps\n",
    "# this doesn't really matter for actual training, but it's nice to have clean timestamps\n",
    "walking_norm = normalize_time_stamps(walking)\n",
    "for user in walking_norm.user.unique():\n",
    "    print(walking_norm.loc[walking_norm.user == user, 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale x,y,z axes\n",
    "scaler = StandardScaler()\n",
    "walking_norm[['x-axis', 'y-axis', 'z-axis']] = scaler.fit_transform(walking_norm[['x-axis', 'y-axis', 'z-axis']].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize 2 second windows with 50% overlap \n",
    "new_features = []\n",
    "labels = []\n",
    "\n",
    "# 20 HZ sampling means 20 samples every second. Therefore 2 seconds is 40 samples, .\n",
    "step_size = 20\n",
    "window_size = 40\n",
    "\n",
    "for user in walking_norm.user.unique():\n",
    "    user_walking_data = walking_norm[walking_norm.user == user]\n",
    "\n",
    "    # go through each users walking data and transform it into \n",
    "    for window_start in range(0, len(user_walking_data) - window_size, step_size):\n",
    "        x = user_walking_data['x-axis'].values[window_start: window_start + window_size]\n",
    "        y = user_walking_data['y-axis'].values[window_start: window_start + window_size]\n",
    "        z = user_walking_data['z-axis'].values[window_start: window_start + window_size]\n",
    "        new_features.append([x, y, z])\n",
    "\n",
    "        # label for a data window is just the user\n",
    "        labels.append(user)\n",
    "\n",
    "# Convert to numpy\n",
    "new_features = np.asarray(new_features, dtype=np.float32).transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform preprocessing \n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "def preprocess_xyz(features, labels):\n",
    "    \n",
    "    # array to hold new vector\n",
    "    processed_features = []\n",
    "    \n",
    "    # columns for each new feature that's derived from x,y,z accelerometer data\n",
    "    new_columns = ['bias',\n",
    "               'x_mean', 'y_mean', 'z_mean', \n",
    "               'x_median', 'y_median', 'z_median', \n",
    "               'magnitude',\n",
    "               'xz_cor', 'yz_cor',\n",
    "               'x_peaks', 'y_peaks', 'z_peaks', 'avg_peaks',\n",
    "               'x_peak_width_mean', 'y_peak_width_mean', 'z_peak_width_mean'\n",
    "              ]\n",
    "    for row in features:\n",
    "        \n",
    "        # obtain x,y,z axes data for particular window\n",
    "        x = row[:,0]\n",
    "        y = row[:,1]\n",
    "        z = row[:,2]\n",
    "        \n",
    "        # find the peaks of height greater than 0.5 for x,y,z axes\n",
    "        x_peaks, _ = find_peaks(x, height=0.1)\n",
    "        y_peaks, _ = find_peaks(y, height=0.1)\n",
    "        z_peaks, _ = find_peaks(z, height=0.1)\n",
    "        \n",
    "        # obtain average width of peaks from base for x,y,z axes\n",
    "        x_peak_widths = peak_widths(x, x_peaks, rel_height=0)\n",
    "        y_peak_widths = peak_widths(y, y_peaks, rel_height=0)\n",
    "        z_peak_widths = peak_widths(z, z_peaks, rel_height=0)\n",
    "        \n",
    "        # 16 time domain features per row \n",
    "        # we may want to include these same features for the frequency domain\n",
    "        new_row = [1,\n",
    "                   np.average(x), np.average(y), np.average(z),\n",
    "                   np.median(x),  np.median(y),  np.median(z),\n",
    "                   LA.norm(row),\n",
    "                   np.average(x)/np.average(z), np.average(y)/np.average(z),\n",
    "                   len(x_peaks), len(y_peaks), len(z_peaks), len(x_peaks)+len(y_peaks)+len(z_peaks)/3,\n",
    "                   np.average(x_peak_widths), np.average(y_peak_widths), np.average(z_peak_widths)\n",
    "                   ]\n",
    "        \n",
    "        # append the new feature row to the data set\n",
    "        processed_features.append(new_row)\n",
    "\n",
    "        \n",
    "    # create dataframe so we can see what's going on in the data\n",
    "    processed_data = pd.DataFrame(processed_features, columns=new_columns)\n",
    "\n",
    "    # append column of users\n",
    "    processed_data['user'] = labels\n",
    "    \n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:33: PeakPropertyWarning: some peaks have a width of 0\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:34: PeakPropertyWarning: some peaks have a width of 0\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:35: PeakPropertyWarning: some peaks have a width of 0\n",
      "/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# process our data\n",
    "feature_vector = preprocess_xyz(new_features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows that there are some data points that are homogenous where no movement took\n",
    "# place within the two seconds. Maybe widening the window makes sense then? If not ...\n",
    "feature_vector = feature_vector.dropna(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>x_mean</th>\n",
       "      <th>y_mean</th>\n",
       "      <th>z_mean</th>\n",
       "      <th>x_median</th>\n",
       "      <th>y_median</th>\n",
       "      <th>z_median</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>xz_cor</th>\n",
       "      <th>yz_cor</th>\n",
       "      <th>x_peaks</th>\n",
       "      <th>y_peaks</th>\n",
       "      <th>z_peaks</th>\n",
       "      <th>avg_peaks</th>\n",
       "      <th>x_peak_width_mean</th>\n",
       "      <th>y_peak_width_mean</th>\n",
       "      <th>z_peak_width_mean</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.313078</td>\n",
       "      <td>0.222490</td>\n",
       "      <td>0.015990</td>\n",
       "      <td>1.390795</td>\n",
       "      <td>0.155498</td>\n",
       "      <td>-0.061796</td>\n",
       "      <td>11.500065</td>\n",
       "      <td>82.119843</td>\n",
       "      <td>13.914487</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>11.750922</td>\n",
       "      <td>10.624316</td>\n",
       "      <td>10.888547</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.234022</td>\n",
       "      <td>0.226213</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>1.150599</td>\n",
       "      <td>0.170846</td>\n",
       "      <td>-0.085984</td>\n",
       "      <td>10.769528</td>\n",
       "      <td>69.312035</td>\n",
       "      <td>12.705831</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>8.444030</td>\n",
       "      <td>8.303156</td>\n",
       "      <td>9.996830</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.231690</td>\n",
       "      <td>0.279675</td>\n",
       "      <td>0.048478</td>\n",
       "      <td>1.176405</td>\n",
       "      <td>0.207227</td>\n",
       "      <td>-0.057398</td>\n",
       "      <td>10.541178</td>\n",
       "      <td>25.406996</td>\n",
       "      <td>5.769070</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>11.572790</td>\n",
       "      <td>11.591530</td>\n",
       "      <td>11.190896</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.435608</td>\n",
       "      <td>0.262821</td>\n",
       "      <td>0.061562</td>\n",
       "      <td>1.368959</td>\n",
       "      <td>0.205522</td>\n",
       "      <td>-0.085984</td>\n",
       "      <td>11.984586</td>\n",
       "      <td>23.319801</td>\n",
       "      <td>4.269222</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.176370</td>\n",
       "      <td>11.102747</td>\n",
       "      <td>11.506022</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.270300</td>\n",
       "      <td>0.222689</td>\n",
       "      <td>0.081462</td>\n",
       "      <td>1.224047</td>\n",
       "      <td>0.128782</td>\n",
       "      <td>-0.085984</td>\n",
       "      <td>11.175553</td>\n",
       "      <td>15.593829</td>\n",
       "      <td>2.733660</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>11.600848</td>\n",
       "      <td>10.337115</td>\n",
       "      <td>9.318982</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12915</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.404806</td>\n",
       "      <td>0.678041</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>-1.315874</td>\n",
       "      <td>0.798978</td>\n",
       "      <td>0.059143</td>\n",
       "      <td>14.253654</td>\n",
       "      <td>-6.906954</td>\n",
       "      <td>3.333698</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>5.604773</td>\n",
       "      <td>10.922097</td>\n",
       "      <td>10.347211</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12917</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.212749</td>\n",
       "      <td>0.581377</td>\n",
       "      <td>-0.115669</td>\n",
       "      <td>-1.255329</td>\n",
       "      <td>0.575579</td>\n",
       "      <td>-0.230011</td>\n",
       "      <td>12.022221</td>\n",
       "      <td>10.484677</td>\n",
       "      <td>-5.026227</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>14.093359</td>\n",
       "      <td>9.957050</td>\n",
       "      <td>11.790576</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12918</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.188034</td>\n",
       "      <td>0.533343</td>\n",
       "      <td>-0.095219</td>\n",
       "      <td>-1.064760</td>\n",
       "      <td>0.486333</td>\n",
       "      <td>-0.129961</td>\n",
       "      <td>11.644744</td>\n",
       "      <td>12.476861</td>\n",
       "      <td>-5.601229</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.059861</td>\n",
       "      <td>9.771859</td>\n",
       "      <td>10.055181</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12919</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.220937</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>-1.332747</td>\n",
       "      <td>0.625034</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>11.818589</td>\n",
       "      <td>-151.222321</td>\n",
       "      <td>72.518410</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.044476</td>\n",
       "      <td>9.852585</td>\n",
       "      <td>10.827445</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12920</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.227538</td>\n",
       "      <td>0.629809</td>\n",
       "      <td>0.086464</td>\n",
       "      <td>-1.129276</td>\n",
       "      <td>0.773398</td>\n",
       "      <td>-0.074989</td>\n",
       "      <td>11.708997</td>\n",
       "      <td>-14.197065</td>\n",
       "      <td>7.284040</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.062590</td>\n",
       "      <td>8.758916</td>\n",
       "      <td>10.888189</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8040 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias    x_mean    y_mean    z_mean  x_median  y_median  z_median  \\\n",
       "0         1  1.313078  0.222490  0.015990  1.390795  0.155498 -0.061796   \n",
       "1         1  1.234022  0.226213  0.017804  1.150599  0.170846 -0.085984   \n",
       "2         1  1.231690  0.279675  0.048478  1.176405  0.207227 -0.057398   \n",
       "3         1  1.435608  0.262821  0.061562  1.368959  0.205522 -0.085984   \n",
       "4         1  1.270300  0.222689  0.081462  1.224047  0.128782 -0.085984   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "12915     1 -1.404806  0.678041  0.203390 -1.315874  0.798978  0.059143   \n",
       "12917     1 -1.212749  0.581377 -0.115669 -1.255329  0.575579 -0.230011   \n",
       "12918     1 -1.188034  0.533343 -0.095219 -1.064760  0.486333 -0.129961   \n",
       "12919     1 -1.220937  0.585498  0.008074 -1.332747  0.625034 -0.006824   \n",
       "12920     1 -1.227538  0.629809  0.086464 -1.129276  0.773398 -0.074989   \n",
       "\n",
       "       magnitude      xz_cor     yz_cor  x_peaks  y_peaks  z_peaks  avg_peaks  \\\n",
       "0      11.500065   82.119843  13.914487       10        7        5  18.666667   \n",
       "1      10.769528   69.312035  12.705831        9        7        4  17.333333   \n",
       "2      10.541178   25.406996   5.769070        8        5        4  14.333333   \n",
       "3      11.984586   23.319801   4.269222        9        8        6  19.000000   \n",
       "4      11.175553   15.593829   2.733660       10        8        7  20.333333   \n",
       "...          ...         ...        ...      ...      ...      ...        ...   \n",
       "12915  14.253654   -6.906954   3.333698        1        9        7  12.333333   \n",
       "12917  12.022221   10.484677  -5.026227        1        8        5  10.666667   \n",
       "12918  11.644744   12.476861  -5.601229        2        8        6  12.000000   \n",
       "12919  11.818589 -151.222321  72.518410        2        8        6  12.000000   \n",
       "12920  11.708997  -14.197065   7.284040        1        7        6  10.000000   \n",
       "\n",
       "       x_peak_width_mean  y_peak_width_mean  z_peak_width_mean  user  \n",
       "0              11.750922          10.624316          10.888547   669  \n",
       "1               8.444030           8.303156           9.996830   669  \n",
       "2              11.572790          11.591530          11.190896   669  \n",
       "3              10.176370          11.102747          11.506022   669  \n",
       "4              11.600848          10.337115           9.318982   669  \n",
       "...                  ...                ...                ...   ...  \n",
       "12915           5.604773          10.922097          10.347211   679  \n",
       "12917          14.093359           9.957050          11.790576   679  \n",
       "12918           9.059861           9.771859          10.055181   679  \n",
       "12919           9.044476           9.852585          10.827445   679  \n",
       "12920           4.062590           8.758916          10.888189   679  \n",
       "\n",
       "[8040 rows x 18 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview dataframe\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many users are left after drop NA\n",
    "len(feature_vector['user'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of there being no movement in a window \n",
    "# (when it's just the 7 with more than 15 mins of data)\n",
    "import matplotlib.pyplot as plt\n",
    "row = new_features[-100]\n",
    "x = row[:,0]\n",
    "peaks, _ = find_peaks(x, height=0.0)\n",
    "plt.plot(x)\n",
    "plt.plot(peaks, x[peaks], \"x\")\n",
    "plt.plot(np.zeros_like(x), \"--\", color=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# an example where there is movement in a window\n",
    "row = new_features[1500]\n",
    "x = row[:,0]\n",
    "peaks, _ = find_peaks(x, height=0.0)\n",
    "plt.plot(x)\n",
    "plt.plot(peaks, x[peaks], \"x\")\n",
    "plt.plot(np.zeros_like(x), \"--\", color=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users before pruning: 12\n",
      "users after pruning: 10\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Then it's clear that there are some useless data points in this set because \n",
    "there is no movement. So we'll get rid of it.\n",
    "'''\n",
    "useful_feature_vector = feature_vector[feature_vector['avg_peaks'] != 0]\n",
    "\n",
    "# max number of dropped windows, 20%\n",
    "max_users = 10\n",
    "cuttoff = minDataPoints//40\n",
    "cuttoff -= cuttoff//10 * 2\n",
    "\n",
    "# num users before pruning\n",
    "print(\"users before pruning: \" + str(len(useful_feature_vector.user.unique())))\n",
    "\n",
    "# pruning after windowing\n",
    "unique_users = useful_feature_vector.user.unique()\n",
    "\n",
    "for i in unique_users:\n",
    "    # if there are less features than the cutoff, then get rid of that user\n",
    "    if len(useful_feature_vector[useful_feature_vector.user == i]) < cuttoff: \n",
    "        useful_feature_vector = useful_feature_vector[useful_feature_vector.user != i]\n",
    "    else:\n",
    "        # otherwise let's get rid of the excess features\n",
    "        useful_feature_vector = useful_feature_vector.drop(useful_feature_vector[useful_feature_vector.user == i][cuttoff:].index)\n",
    "    \n",
    "        \n",
    "unique_users = useful_feature_vector.user.unique()\n",
    "\n",
    "for i in range(0, len(unique_users)):\n",
    "    # if there are more than the max users, get rid of them\n",
    "    if i > max_users - 1:\n",
    "        useful_feature_vector = useful_feature_vector[useful_feature_vector.user != unique_users[i]]\n",
    "print(\"users after pruning: \" + str(len(useful_feature_vector.user.unique())))\n",
    "\n",
    "    \n",
    "\n",
    "# now our training set and labels are ready to be used\n",
    "X = useful_feature_vector.drop(columns=['user']).to_numpy()\n",
    "y = useful_feature_vector['user'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Prepare Models</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# foud work around for sklearn on medium\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "def print_metrics(y_test, preds, model_name):\n",
    "    cm = pd.crosstab(y_test, preds, rownames=['Actual User'], colnames=['Predicted User'])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(cm, annot=True, fmt='g')\n",
    "    \n",
    "    recall = recall_score(y_test, preds, average='macro')\n",
    "    \n",
    "    print(\"TOTAL \" + model_name + \" RECALL: \", recall)\n",
    "    \n",
    "    auc = multiclass_roc_auc_score(y_test, preds)\n",
    "    print(\"TOTAL \" + model_name + \" AUC: \", auc)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    print(\"TOTAL \" + model_name + \" ACCURACY: \", accuracy)\n",
    "    print(\"---------------------- classification report ----------------------\")\n",
    "    classes = np.unique(y_test)\n",
    "    classes = list(map(str, classes.tolist()))\n",
    "    cr = classification_report(y_test, preds, target_names=classes)\n",
    "    print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's try a RF (Random Forest)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def random_forest(X_train, y_train, X_test, y_test, metrics=False):\n",
    "    classifier = RandomForestClassifier(n_jobs=2, random_state=0, n_estimators=10)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    preds = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    if metrics:\n",
    "        print_metrics(y_test, preds, \"Random Forest\")\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def decision_tree(X_train, y_train, X_test, y_test, metrics=False):\n",
    "\n",
    "    classifier = DecisionTreeClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    preds = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    if metrics:\n",
    "        print_metrics(y_test, preds, \"Decision Tree\")\n",
    "        \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def support_vector_machine(X_train, y_train, X_test, y_test, metrics=False):\n",
    "    classifier = svm.SVC(gamma='auto')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    preds = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    if metrics:\n",
    "        print_metrics(y_test, preds, \"Support Vector Machine\")\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression(X_train, y_train, X_test, y_test, metrics=False):\n",
    "    classifier = LogisticRegression(random_state=0, \n",
    "                                    solver='newton-cg',\n",
    "                                    multi_class='auto',\n",
    "                                    max_iter=1200).fit(X_train, y_train)\n",
    "    preds = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    if metrics:\n",
    "        print_metrics(y_test, preds, \"Linear Regression\")\n",
    "        \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL Random Forest RECALL:  0.9635416666666666\n",
      "TOTAL Random Forest AUC:  0.9797453703703705\n",
      "TOTAL Random Forest ACCURACY:  0.9635416666666666\n",
      "---------------------- classification report ----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         585       0.92      0.97      0.94        96\n",
      "         602       0.97      0.98      0.97        96\n",
      "         603       0.99      0.95      0.97        96\n",
      "         608       0.96      1.00      0.98        96\n",
      "         641       0.91      0.96      0.93        96\n",
      "         648       0.94      0.92      0.93        96\n",
      "         669       0.97      0.94      0.95        96\n",
      "         675       1.00      1.00      1.00        96\n",
      "         688       1.00      1.00      1.00        96\n",
      "        1750       0.99      0.93      0.96        96\n",
      "\n",
      "    accuracy                           0.96       960\n",
      "   macro avg       0.96      0.96      0.96       960\n",
      "weighted avg       0.96      0.96      0.96       960\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9635416666666666"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quickly test models\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "              X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# looks like we're in business\n",
    "random_forest(X_train, y_train, X_test, y_test, True)\n",
    "#decision_tree(X_train, y_train, X_test, y_test, True)\n",
    "#support_vector_machine(X_train, y_train, X_test, y_test, True)\n",
    "#logistic_regression(X_train, y_train, X_test, y_test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>K-Fold Cross Validation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.940625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.915625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-314cd96c6189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-841ce45583da>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(X_train, y_train, X_test, y_test, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                     \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                     max_iter=500).fit(X_train, y_train)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1604\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m                       sample_weight=sample_weight)\n\u001b[0;32m-> 1606\u001b[0;31m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             w0, n_iter_i = newton_cg(hess, func, grad, w0, args=args,\n\u001b[0;32m--> 954\u001b[0;31m                                      maxiter=max_iter, tol=tol)\n\u001b[0m\u001b[1;32m    955\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             coef_, intercept_, n_iter_i, = _fit_liblinear(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36mnewton_cg\u001b[0;34m(grad_hess, func, grad, x0, args, tol, maxiter, maxinner, line_search, warn)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# Inner loop: solve the Newton update by conjugate gradient, to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# avoid inverting the Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mxsupi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfhess_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtermcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0malphak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36m_cg\u001b[0;34m(fhess_p, fgrad, maxiter, tol)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mAp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhess_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# check curvature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mcurv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mhessp\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mhessProd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mhessProd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_yhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mhessProd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "number_of_splits = 10\n",
    "total_accuracy = 0\n",
    "\n",
    "# perform kfold cross validation\n",
    "kf = StratifiedKFold(n_splits=number_of_splits, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # train model\n",
    "    \n",
    "    # random forest\n",
    "    #accuracy = random_forest(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # decision tree\n",
    "    #accuracy = decision_tree(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # svm\n",
    "    #accuracy = support_vector_machine(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # logistic regression\n",
    "    accuracy = logistic_regression(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "print(\"Accuracy after kfold cross validation is: \" + str(total_accuracy/number_of_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "import random\n",
    "import math\n",
    "import torch.utils.data as tdata\n",
    "import torch.optim as opt\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [9, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size_train = 64\n",
    "batch_size_test = 512\n",
    "learning_rate = 0.001\n",
    "logging_interval = 1\n",
    "valid_percent = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
